# SOTA Vision/OCR Models for Book Digitization: 2025 Benchmark Analysis

**Multimodal LLMs now rival dedicated OCR services for document extraction, with Qwen2.5-VL matching GPT-4o accuracy at 90% lower cost.** Mistral's purpose-built OCR API achieves 95% benchmark accuracy on scanned documents—outperforming both traditional OCR engines and general-purpose VLMs—while charging just $1-2 per 1,000 pages. For English book OCR at scale, the optimal strategy combines a high-volume cost-effective model (Gemini 2.0 Flash or Qwen2.5-VL) with escalation to Mistral OCR 3 or Claude 3.5 Sonnet for complex pages.

---

## Shortlist: 10 API-accessible models for book OCR

The following models were selected based on OCR benchmark performance, API availability, pricing, and suitability for scanned book pages with multi-column layouts and tables.

### Tier 1: Purpose-built document OCR

| Model | Provider | Release | Pricing | OCR Performance | Best For |
|-------|----------|---------|---------|-----------------|----------|
| **Mistral OCR 3** | Mistral AI | Dec 18, 2025 | $2/1K pages ($1 batch) | 74% win rate vs GPT-4o on complex docs; 98.96% on scanned documents | Complex tables, handwriting, degraded scans |
| **Mistral OCR 2** | Mistral AI | Mar 6, 2025 | $1/1K pages ($0.50 batch) | 94.89% overall; 2,000 pages/min throughput | High-volume production OCR |
| **Google Document AI** | Google Cloud | 2024 (v2) | $1.50/1K pages | 98%+ accuracy; best-in-class on noisy docs | Enterprise document processing |
| **AWS Textract** | AWS | Ongoing | $1.50/1K pages (basic) | 97-99% on printed text; 2.8% WER | AWS ecosystem integration |

### Tier 2: Multimodal LLMs with strong OCR

| Model | Provider | Release | Pricing | OCR Performance | Best For |
|-------|----------|---------|---------|-----------------|----------|
| **Qwen2.5-VL-72B** | Alibaba (via Fireworks) | Jan 2025 | $0.90/1M tokens | **96.4% DocVQA** (SOTA), 88.8% OCRBench | Cost-effective high-quality OCR |
| **Gemini 2.0 Flash** | Google | Feb 5, 2025 (GA) | $0.10/1M input tokens | 59.3% OCRBench v2 (English); 93.7% extraction score | Ultra-low-cost batch processing |
| **Claude 3.5 Sonnet** | Anthropic | Oct 2024 (updated) | $3/1M input tokens | 86.9% extraction; best format consistency | Complex layout preservation |
| **GPT-4o** | OpenAI | May 13, 2024 | $2.50/1M input tokens | 47.6% OCRBench v2; strong on standard docs | General-purpose, reliable baseline |

### Tier 3: Cost-optimized options

| Model | Provider | Release | Pricing | OCR Performance | Best For |
|-------|----------|---------|---------|-----------------|----------|
| **GPT-4o-mini** | OpenAI | Jul 18, 2024 | $0.15/1M input tokens | Lower than GPT-4o but acceptable for clean scans | Budget baseline, simple layouts |
| **Qwen2.5-VL-7B** | Alibaba (via Fireworks) | Jan 2025 | $0.20/1M tokens | 88.4% DocVQA; outperforms Llama 3.2-11B | Lowest cost with good accuracy |

---

## Detailed model analysis

### Mistral OCR 3 sets the new document understanding benchmark

Mistral's dedicated OCR service claims **74% win rate against GPT-4o and Gemini** on forms, scanned documents, and complex tables. Internal benchmarks show **98.96% accuracy on scanned documents** and **96.12% on tables**—substantially outperforming general-purpose VLMs. The model handles cursive handwriting, mixed-content annotations, compression artifacts, skew, and low DPI with notable robustness.

Key differentiators include HTML table reconstruction with proper colspan/rowspan handling, structured Markdown output with embedded image preservation, and the "doc-as-prompt" capability for targeted extraction. Maximum file size is 50MB with 1,000 pages per request. At **$2 per 1,000 pages** (or $1 via batch API), it offers predictable per-page pricing versus token-based uncertainty.

**Limitations:** Closed-source, no self-hosting except for enterprise. Per-page pricing may be costlier than token-based models for simple documents with minimal text.

### Qwen2.5-VL delivers SOTA accuracy at radically lower cost

The **72B variant achieves 96.4% on DocVQA**—the highest score among public models—and 88.8% on OCRBench. On OCRBench v2 English tasks, it scores **63.7%**, more than 20 points above GPT-4o's 47.6%. The model generates structured JSON outputs for invoices and forms, handles dynamic resolutions up to 2304×2304, and supports multilingual OCR across European languages, Japanese, Korean, and Arabic.

Available via Fireworks.ai at **$0.90 per 1M tokens** (32B/72B) or Together.ai, it costs roughly **$0.05-0.20 per page** depending on image resolution and output length. The 7B variant at $0.20/1M tokens offers 88.4% DocVQA accuracy—matching GPT-4o on many tasks at **10-15× lower cost**.

**Limitations:** 72B requires on-demand deployment on Fireworks (not serverless). Token costs vary significantly by image resolution. No function calling support.

### Gemini 2.0 Flash offers unmatched throughput economics

At **$0.10 per 1M input tokens** (or $0.05 batch), Gemini 2.0 Flash provides the lowest cost per page among frontier models—roughly **$0.01-0.05 per page** for typical book scans. It handles 1M token context windows, enabling processing of entire book sections in single calls. Benchmark performance shows **93.7% on extraction tasks** and strong chart/table understanding.

The model achieves **2× the speed of Gemini 1.5 Pro** while outperforming it on document benchmarks. Native PDF understanding eliminates the need for preprocessing, and Google Search grounding can provide citation verification.

**Limitations:** OCRBench v2 score of 59.3% lags behind specialized OCR services. Knowledge cutoff (June 2024) limits understanding of recent document formats. Inferior to 1.5 Pro on very long context evaluation.

### Claude 3.5 Sonnet excels at format preservation

Claude demonstrates **86.9% extraction accuracy** on OCRBench v2 with the **best format consistency** among tested LLMs. Independent evaluations show it accurately transcribes text from imperfect images while preserving document structure. At 72.3 tokens/second with 0.97s latency, it balances quality with reasonable throughput.

The October 2024 update improved visual reasoning significantly. Claude particularly excels at interpreting charts, graphs, and maintaining the logical structure of complex multi-column layouts during extraction.

**Limitations:** At $3/1M input tokens, it costs 30× more than Gemini 2.0 Flash. Knowledge cutoff is April 2024. Content policies may block certain document types.

### Dedicated OCR services provide reliability at scale

**AWS Textract** offers the most granular pricing: basic OCR at **$1.50 per 1,000 pages**, with table extraction at $15/1K and form extraction at $50/1K. The Layout feature detects paragraphs, headers, and reading order—critical for book digitization. Integration with S3, Lambda, and Step Functions enables robust production pipelines. However, it supports only 6 languages and has no vertical text extraction.

**Google Document AI** achieved **98% accuracy with 2.0% WER**—the lowest word error rate in comparative benchmarks. It substantially outperformed Tesseract on degraded documents and handles 200+ languages. The Enterprise Document OCR Processor (v2) at $1.50/1K pages provides production-grade reliability.

**Azure Document Intelligence** v4.0 (November 2024) offers **native Markdown output** and strong table preservation. Its 40% price reduction on custom extraction (June 2024) brought it to $30/1K pages. It leads on older printed documents but trails Google on handwriting.

---

## Benchmark synthesis: how models compare on book-relevant tasks

The **OCRBench v2 leaderboard** (June 2025) reveals significant performance gaps:

| Capability | Leader | Score | GPT-4o Score |
|------------|--------|-------|--------------|
| Overall (English) | Seed1.6-vision | 62.2% | 47.6% |
| Extraction | Gemini 2.5 Pro | 93.7% | 87.4% |
| Recognition | Gemini 2.5 Pro | 70.9% | 58.6% |
| Table understanding | Qwen3-Omni-30B | High | Moderate |

The **OmniAI real-world benchmark** (1,000 documents) found Qwen2.5-VL-72B and GPT-4o both achieved ~75% accuracy, while Mistral OCR scored 72.2%—demonstrating that open-weight models now compete with proprietary solutions.

For **degraded historical scans**, IBM Granite Vision 3.3 2B specifically trained on low-quality documents achieved #2 on OCRBench. Research on PreP-OCR pipelines shows image restoration + post-correction significantly improves results on aged book pages.

---

## Cost comparison for 1 million book pages

| Model | Base Cost | With 50% Batch | Notes |
|-------|-----------|----------------|-------|
| Gemini 2.0 Flash | ~$100-500 | ~$50-250 | Varies by page complexity |
| Qwen2.5-VL-7B | ~$200-1,000 | ~$100-500 | Token-based variability |
| Mistral OCR 2 | $1,000 | $500 | Fixed per-page pricing |
| AWS Textract | $1,500 | N/A | Basic OCR only |
| Google Document AI | $1,500 | N/A | Enterprise OCR |
| Mistral OCR 3 | $2,000 | $1,000 | Best accuracy |
| Claude 3.5 Sonnet | ~$3,000-10,000 | ~$1,500-5,000 | Highest per-token cost |

---

## Known limitations and failure modes

**Multi-column layouts** remain challenging across all models. VLMs occasionally merge columns or misorder paragraphs. Mistral OCR and AWS Textract Layout feature handle this best through explicit structure detection.

**OCR hallucinations** occur in all VLMs. Common patterns include character-level misreading (similar shapes), word substitution, and fabrication of text that doesn't exist. GPT-4V achieved only 38.3% on VHTest hallucination benchmarks. Structured JSON output modes and post-OCR verification reduce but don't eliminate this risk.

**Handwritten text** shows the widest accuracy variance. OCRBench v2 rates GPT-5, olmOCR-2-7B, and Gemini 2.5 Pro as top performers. Traditional dedicated OCR services (Textract, Azure) struggle with cursive handwriting.

**Content policy blocks** affect OpenAI models specifically—GPT-4o refuses to process photo IDs and passports, which may unexpectedly fail on books containing such reproductions.

---

## Top 3 recommendations

### 1. Mistral OCR 3 — best accuracy for production workloads

**Rationale:** Purpose-built for document extraction with **98.96% accuracy on scanned documents**. Handles the exact use case (books with tables, multi-column text) with explicit layout preservation and structured Markdown output. Predictable **$1 per 1,000 pages** (batch) pricing eliminates token-counting complexity. The only model to claim explicit superiority over both GPT-4o and Gemini on complex document tasks.

**Use for:** Primary OCR engine for high-quality extraction, complex layouts, and pages with tables or handwriting.

### 2. Qwen2.5-VL-72B (via Fireworks.ai) — best value for high accuracy

**Rationale:** Achieves **96.4% DocVQA** (highest among public models) at ~**$0.10-0.20 per page**—roughly 10× cheaper than Mistral OCR while approaching its accuracy on standard documents. Open-weight model provides transparency, and hosted API eliminates infrastructure complexity. Excellent fallback when Mistral OCR 3's per-page pricing is cost-prohibitive for cleaner documents.

**Use for:** High-volume processing of relatively clean scans where cost optimization is critical.

### 3. Gemini 2.0 Flash — lowest cost high-volume processing

**Rationale:** At **$0.05 input tokens per 1M tokens** (batch), it costs 20× less than Qwen2.5-VL while still achieving strong extraction scores. The 1M token context window enables processing entire book chapters in single calls. Acceptable accuracy for first-pass OCR with escalation to higher-quality models for complex pages.

**Use for:** Initial OCR pass in a tiered pipeline; processing large volumes where some accuracy trade-off is acceptable.

---

## Recommended pipeline architecture

A **tiered confidence-based approach** can optimize cost/quality:

1. **First pass:** Gemini 2.0 Flash (~$0.02/page) for all pages
2. **Confidence check:** Evaluate OCR confidence scores and layout complexity
3. **Escalation:** Route low-confidence or complex pages to Mistral OCR 3 ($0.001-0.002/page)
4. **Verification:** For critical text, cross-reference with Qwen2.5-VL output

This architecture could process 1M pages for **$20,000-50,000** (depending on escalation rate) while achieving near-maximum accuracy, compared to $1-2M using a traditional OCR ensemble.

---

## Evidence appendix

### Release dates
- Mistral OCR 3: https://mistral.ai/news/mistral-ocr-3 (Dec 18, 2025)
- Mistral OCR 2: https://mistral.ai/news/mistral-ocr (Mar 6, 2025)
- Qwen2.5-VL: https://qwenlm.github.io/blog/qwen2.5-vl/ (Jan 2025)
- Gemini 2.0 Flash: https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/ (Feb 5, 2025 GA)
- GPT-4o: https://openai.com/index/hello-gpt-4o/ (May 13, 2024)
- Claude 3.5 Sonnet: https://www.anthropic.com/news/claude-3-5-sonnet (Jun 20, 2024; Oct 2024 update)

### Pricing pages
- Mistral: https://mistral.ai/news/mistral-ocr-3
- OpenAI: https://openai.com/api/pricing/
- Google Gemini: https://ai.google.dev/gemini-api/docs/pricing
- Anthropic: https://www.anthropic.com/pricing
- AWS Textract: https://aws.amazon.com/textract/pricing/
- Google Document AI: https://cloud.google.com/document-ai/pricing
- Fireworks.ai: https://fireworks.ai/pricing

### Benchmarks
- OCRBench v2 leaderboard: https://99franklin.github.io/ocrbench_v2/
- DocVQA: https://www.docvqa.org/
- OmniAI benchmark: https://getomni.ai/blog/ocr-benchmark
- AIMultiple OCR accuracy: https://research.aimultiple.com/ocr-accuracy/
- InternVL2 results: https://internvl.github.io/blog/2024-07-02-InternVL-2.0/

### Model documentation
- Mistral OCR docs: https://docs.mistral.ai/capabilities/document_ai/basic_ocr
- Qwen2.5-VL: https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct
- AWS Textract: https://docs.aws.amazon.com/textract/
- Google Document AI: https://cloud.google.com/document-ai/docs